{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ca9634-72eb-46c9-9da7-77931052b680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports and Datasets Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba098781-d34e-4fc5-a4ca-43c422b5ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, r2_score, mean_squared_error\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8733cac-7e3a-40e2-9b0b-7a1a6440c9c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Titanic Dataset for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12adc4e6-100a-4d61-bec4-a32bdabf85e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data = pd.read_csv('datasets/titanic/titanic_dataset.csv')\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861ba0ce-d62c-4859-b85d-73e502be2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, validation and test data: 70:15:15 division\n",
    "# use random seed to have always the same split if we make different analyses\n",
    "\n",
    "training_data_clas = titanic_data.sample(frac=0.7, random_state=1234)\n",
    "data_without_train_clas = titanic_data.drop(training_data_clas.index)\n",
    "validation_data_clas = data_without_train_clas.sample(frac=0.5, random_state=27)\n",
    "test_data_clas = data_without_train_clas.drop(validation_data_clas.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd752337-126a-4443-a0c6-3078ff5b5c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split:\n",
      "training data: 624\n",
      "validation data: 134\n",
      "test data: 133\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset split:\\ntraining data: {(len(training_data_clas))}\\nvalidation data: {(len(validation_data_clas))}\\ntest data: {(len(test_data_clas))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8a5999-7a28-467b-a806-589ff6056841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7358490566037735"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for curiosity check how many female and male survived\n",
    "\n",
    "woman = training_data_clas.loc[training_data_clas.Sex == 'female']['Survived'] # 1: survived, 0: died\n",
    "rate_woman_survived = sum(woman) / len(woman)\n",
    "rate_woman_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1588326f-b21d-4c4b-9cdf-863b7dea66d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19174757281553398"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men = training_data_clas.loc[training_data_clas.Sex == 'male']['Survived']\n",
    "rate_men_survived = sum(men) / len(men)\n",
    "rate_men_survived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6210e09d-2cd3-472e-80e8-25a4585efad6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Features and Target preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6dc711-f6b0-4257-ab90-9f7f9c9aea01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523    1\n",
       "778    0\n",
       "760    0\n",
       "496    1\n",
       "583    0\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the prediction to learn\n",
    "                \n",
    "y = training_data_clas['Survived'] # Target of training set\n",
    "y_val = validation_data_clas['Survived'] # Target of validation set\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cacdc4f-7518-4566-9c39-5ff57649557a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  SibSp  Parch  Sex_female  Sex_male\n",
       "523       1      0      1           1         0\n",
       "778       3      0      0           0         1\n",
       "760       3      0      0           0         1\n",
       "496       1      1      0           1         0\n",
       "583       1      0      0           0         1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the features vector, where we create dummies values to be evaluated\n",
    "\n",
    "features = ['Pclass', 'Sex', 'SibSp', 'Parch']\n",
    "\n",
    "X = pd.get_dummies(training_data_clas[features])\n",
    "X_val = pd.get_dummies(validation_data_clas[features])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfffbe-5a1f-4583-b1ec-889dfbee1b13",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Boston Housing Dataset for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718fd099-d8d0-44a6-b16f-0997225f0043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_housing = pd.read_csv('datasets/boston_housing/BostonHousing.csv')\n",
    "boston_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c289c987-3241-440a-b362-8d7511a3b081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   crim     506 non-null    float64\n",
      " 1   zn       506 non-null    float64\n",
      " 2   indus    506 non-null    float64\n",
      " 3   chas     506 non-null    int64  \n",
      " 4   nox      506 non-null    float64\n",
      " 5   rm       506 non-null    float64\n",
      " 6   age      506 non-null    float64\n",
      " 7   dis      506 non-null    float64\n",
      " 8   rad      506 non-null    int64  \n",
      " 9   tax      506 non-null    int64  \n",
      " 10  ptratio  506 non-null    float64\n",
      " 11  b        506 non-null    float64\n",
      " 12  lstat    506 non-null    float64\n",
      " 13  medv     506 non-null    float64\n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "boston_housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb712fe9-a85c-4205-bb4b-4ced80cc40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_reg = boston_housing.sample(frac=0.7, random_state=1234)\n",
    "data_without_train_reg = boston_housing.drop(training_data_reg.index)\n",
    "validation_data_reg = data_without_train_reg.sample(frac=0.5, random_state=27)\n",
    "test_data_reg = data_without_train_reg.drop(validation_data_reg.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4772a9-b8b8-4339-8b1e-f8268804a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split:\n",
      "training data: 354\n",
      "validation data: 76\n",
      "test data: 76\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset split:\\ntraining data: {(len(training_data_reg))}\\nvalidation data: {(len(validation_data_reg))}\\ntest data: {(len(test_data_reg))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98265fda-a21a-4d9d-977c-5efc5fa63908",
   "metadata": {},
   "source": [
    "### Features and Target preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ade6b6e-a71b-432c-988b-da5d909accf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64     33.0\n",
       "100    27.5\n",
       "400     5.6\n",
       "485    21.2\n",
       "454    14.9\n",
       "Name: medv, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target training and validation\n",
    "                \n",
    "yreg = training_data_reg['medv'] # Target of training set\n",
    "yreg_val = validation_data_reg['medv'] # Target of validation set\n",
    "yreg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48657b75-c425-444f-99f8-1e95dbc377af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.01951</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>7.104</td>\n",
       "      <td>59.5</td>\n",
       "      <td>9.2229</td>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>18.6</td>\n",
       "      <td>393.24</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.14866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>6.727</td>\n",
       "      <td>79.9</td>\n",
       "      <td>2.7778</td>\n",
       "      <td>5</td>\n",
       "      <td>384</td>\n",
       "      <td>20.9</td>\n",
       "      <td>394.76</td>\n",
       "      <td>9.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>25.04610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6930</td>\n",
       "      <td>5.987</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5888</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>26.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>9.51363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>6.728</td>\n",
       "      <td>94.1</td>\n",
       "      <td>2.4961</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>6.68</td>\n",
       "      <td>18.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim    zn  indus  chas     nox     rm    age     dis  rad  tax  \\\n",
       "64    0.01951  17.5   1.38     0  0.4161  7.104   59.5  9.2229    3  216   \n",
       "100   0.14866   0.0   8.56     0  0.5200  6.727   79.9  2.7778    5  384   \n",
       "400  25.04610   0.0  18.10     0  0.6930  5.987  100.0  1.5888   24  666   \n",
       "485   3.67367   0.0  18.10     0  0.5830  6.312   51.9  3.9917   24  666   \n",
       "454   9.51363   0.0  18.10     0  0.7130  6.728   94.1  2.4961   24  666   \n",
       "\n",
       "     ptratio       b  lstat  \n",
       "64      18.6  393.24   8.05  \n",
       "100     20.9  394.76   9.42  \n",
       "400     20.2  396.90  26.77  \n",
       "485     20.2  388.62  10.58  \n",
       "454     20.2    6.68  18.71  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the features vector\n",
    "\n",
    "features_reg = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
    "\n",
    "Xreg = training_data_reg[features_reg]\n",
    "Xreg_val = validation_data_reg[features_reg]\n",
    "Xreg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dce44e-a729-4442-a098-7d096206470c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression & Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea169aa-47d3-4192-a227-05c7bff53abf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad283313-8cd4-4663-9ef4-45cff9e90ce9",
   "metadata": {},
   "source": [
    "Logistic Regression is well-known and widely used statistical classification method. It computes the probability of assigning a class to a sample:\n",
    "$$ P(y_i|X_i) $$\n",
    "For this purpose logistic regression use the sigmoid functions:\n",
    "$$ P(y_i = +1 |X_i) = \\frac{1}{1+e^{-Score(X_i)}} $$\n",
    "where:\n",
    "$$ Score(X_i) = w_1 x_1 + .... + w_n x_x = h_i(X_i) $$\n",
    "<div>\n",
    "<center><img src=\"images/sigmoid.png\" width=\"400\" center=/></center>\n",
    "</div>\n",
    "Logistic Regression search for the weight vector $ W $ which correspond to the highest likelihood:\n",
    "$$ l(W) = \\prod{P(y_i|X_i,W_i)} $$\n",
    "To find the best weights, it performs a gradient ascent on the log likelihood function:\n",
    "$$ ll(W) = \\ln{l(W)} $$\n",
    "Which updates weights $i$ using:\n",
    "$$  \\frac{\\partial ll}{\\partial w_i} = \\sum{h_i(X_i)(1[y_i = +1] - P(y = +1|X_i, W))} $$\n",
    "so the update is made in this way:\n",
    "$$ w_i = w_i - \\alpha\\frac{\\partial ll}{\\partial w_i} $$ \n",
    "where $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2250b12-202a-47ab-89fd-6da05b4ac5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 100,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': None,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from pprint import pprint\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "pprint(lr_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e153c57-c56d-400a-a3e6-ac4e15a5b00c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab89d0-af31-4525-953f-4c6f3708d744",
   "metadata": {},
   "source": [
    "The main parameters of logistic regression are:\n",
    "- solver: is the algorithm to use in the optimization problem\n",
    "    - lbfgs: relatively performs well compared to other methods and it saves a lot of memory, however, sometimes it may have issues with convergence.\n",
    "    - sag faster than other solvers for large datasets, when both the number of samples and the number of features are large.\n",
    "    - saga: the solver of choice for sparse multinomial logistic regression and it’s also suitable for very large datasets.\n",
    "    - newton-cg computationally expensive because of the Hessian Matrix.\n",
    "    -liblinearrecommended when you have a high dimension dataset - solving large-scale classification problems.\n",
    "- penalty: it regulate the overfitting of the model penalizing the weights fitting to much the training data\n",
    "    - l1: lasso\n",
    "    - l2: ridge regression\n",
    "    - elasticnet\n",
    "    - none\n",
    "- C: regularization strength works with the penalty to regulate overfitting\n",
    "    - small values: specify stronger regularization\n",
    "    - high values: less regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c3ad98-ed4f-4752-97e0-54a7544c4202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7686567164179104"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this case use defaults values but we could do a search grid search\n",
    "\n",
    "lr_model = lr_model.fit(X, y)\n",
    "lr_preds = lr_model.predict(X_val)\n",
    "accuracy_score(lr_preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf15def-fe21-478d-940a-d13a7e99a226",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c915844-3173-4e7b-a296-3ccd4abe14e6",
   "metadata": {},
   "source": [
    "Linear regression is a well-known regression model to predict continuous targets:\n",
    "- Given N Samples (X, y), X: the vector of features, y: the target, linear regression computes a model:\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1x_i + ... + w_Nx_i\n",
    "$$\n",
    "- We evaluate the model by computing the RSS (Resdidual Sum of Squared):\n",
    "$$\n",
    "RSS(w_0,...,w_N) = \\sum_{i=1}^{N}{(y_i - \\hat{y_i})^2} = \\sum_{i=1}^{N}{(y_i - (w_0 + w_1x_i + ... + w_Nx_i))^2}\n",
    "$$\n",
    "- The goal is to find the weights that minimize the RSS\n",
    "$$\n",
    "w_0,...,w_N = \\arg\\min_{w_0,...,w_N}{RSS(w_0,...,w_N)}\n",
    "$$\n",
    "- To solve this we have two approach\n",
    "    - $RSS(w_0,...,w_N) = 0$: high computation\n",
    "    - apply gradient descent until convergence\n",
    "I show a graphical simplify example with only one features dimension, the training data:\n",
    "<div>\n",
    "<center><img src=\"images/linear_reg/training_data.png\" width=\"400\" center=/></center>\n",
    "</div>\n",
    "A simple linear regression model an them:\n",
    "<div>\n",
    "<center><img src=\"images/linear_reg/reg_model.png\" width=\"400\" center=/></center>\n",
    "</div>\n",
    "To evaluate the regression problems we are going to use $R^2$ metrics:\n",
    "$$\n",
    "TSS = \\sum_{i=1}^{N}{(y_i - \\hat{y_i})^2}\n",
    "$$\n",
    "$$\n",
    "R^2 = 1 - \\frac{TSS}{RSS}\n",
    "$$\n",
    "R2 measures of how well the regression line approximates the real data points. When R2 is 1, the regression line perfectly fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad0c20f-2175-47ff-acc1-24cadcd147e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'copy_X': True,\n",
      " 'fit_intercept': True,\n",
      " 'n_jobs': None,\n",
      " 'normalize': 'deprecated',\n",
      " 'positive': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_model = LinearRegression()\n",
    "pprint(reg_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88c461e5-7426-4408-b16c-e30d47e6d5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28648680667869086"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.fit(Xreg, yreg)\n",
    "reg_preds = reg_model.predict(Xreg_val)\n",
    "r2_score(reg_preds, yreg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be05764-a5d3-440c-bc67-3f2e79c98513",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f02d85-9bae-438d-945c-37c55b64a307",
   "metadata": {},
   "source": [
    "Naive Bayes method are a set of supervised learning algorithms based on applying Bayes' theorem with the 'naive' assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "<br>\n",
    "Given the target y and the example x described by $n$ attributes, Bayes Theorem says that:\n",
    "$$\n",
    "P(y|X) = \\frac{P(X|y)P(y)}{P(X)}\n",
    "$$\n",
    "As just said, Naive Bayes assume that attributes are statistically independent. Thus, evidence splits into parts that are independent:\n",
    "$$\n",
    "P(y|X) = \\frac{P(x_1|y)...P(x_n|y)P(y)}{P(X)}\n",
    "$$\n",
    "where: \n",
    "- $P(y)$ is the a priori probability of y, which is the probability of event before evidence is seen. So it can be easily computed since it is the then relative frequency of the class $y$ in the training set\n",
    "- $P(C|A)$ is the a posteriori probability of y, which is the probability of event after the evidence is seen\n",
    "<br>\n",
    "In spite of their apparently over-simplified assumptions, Naive Bayes works quite well in many real-world situations. In addition, it can be extremely fast compared to more sophisticated methods.\n",
    "<br>\n",
    "On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "<br><br>\n",
    "Here we are going to analyze:\n",
    "- Gaussian Naive Bayes\n",
    "- Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597051c-86b3-494d-8cfd-b5c8682bac5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7857c1d-cd17-4378-93d2-ac03f39b088e",
   "metadata": {},
   "source": [
    "GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\n",
    "$$\n",
    "P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} e^{(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2})}\n",
    "$$\n",
    "where $\\sigma_y$ and $\\mu_y$ are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71ebc394-1a37-4db7-a237-d1dba689f1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7686567164179104"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model = gnb_model.fit(X, y)\n",
    "gnb_preds = gnb_model.predict(X_val)\n",
    "accuracy_score(gnb_preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36209fb-b0df-4275-a662-36e7607d4bce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b8a30-29cc-4ef9-a477-7793e8553432",
   "metadata": {},
   "source": [
    "MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e10973-1fa7-451e-aacd-601a9cd53aaf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# KNN (K-Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ede9db-c7e9-474c-b490-44a337429bef",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor is one of the simplest form of learning. To decide the label for an unseen sample, consider from the training data which are more similar to the unknown one. The training dataset is the model itself, it is the knowledge, the similarity function defines what's learned.\n",
    "<br>\n",
    "The most notable similarity measure are:\n",
    "- Euclidean distance (Minkowski): the typical function to compute the similarity between two samples:\n",
    "$$\n",
    "d(p,q) = \\sqrt{\\sum{(p_i - q_i)^2}}\n",
    "$$\n",
    "- Manhattan distance: l1 loss\n",
    "$$\n",
    "d(p,q) = {\\sum{|(p_i - q_i)}|}\n",
    "$$\n",
    "- Cosine distance: it is the angle that the vectors to those points make. This angle will be in the range 0 to 180 degrees, regardless of how may dimensions the space has\n",
    "$$\n",
    "d(X,Y) = \\arccos{\\frac{\\sum{x_iy_i}}{\\sqrt{\\sum{x_i^2}}\\sqrt{\\sum{y_i^2}}}}\n",
    "$$\n",
    "- Jaccard distance: it is a measure of how dissimilar two sets are\n",
    "- Hamming distance: it is the number of components in which they differ (e.g. v1=10101, v2=11110, HammingD= 3/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59c37e-ff42-4beb-9eef-8e0afea069c4",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af04c866-5697-45d5-96ae-34d50f1a46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto',\n",
      " 'leaf_size': 30,\n",
      " 'metric': 'minkowski',\n",
      " 'metric_params': None,\n",
      " 'n_jobs': None,\n",
      " 'n_neighbors': 5,\n",
      " 'p': 2,\n",
      " 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "pprint(knn_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d4157-e261-4f43-81f8-677428e2d800",
   "metadata": {},
   "source": [
    "### Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a9c57e-fb82-4544-bb5d-ac2682f91dbc",
   "metadata": {},
   "source": [
    "The main parameters of knn are:\n",
    "- n_neighbors: the number of neighbors to use\n",
    "- metric: the similarity function to use\n",
    "- p: This is the power parameter for the Minkowski metric. When p=1, this is equivalent to using manhattan_distance(l1), and euclidean_distance(l2) for p=2\n",
    "- algorithm: the algorithm used to compute the nearest neighbors:\n",
    "    - ‘ball_tree’ will use BallTree\n",
    "    - ‘kd_tree’ will use KDTree\n",
    "    - ‘brute’ will use a brute-force search.\n",
    "    - ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "- leaf_size: leaf size passed to BallTree or KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ce70e05-4f95-4c29-9f6e-d86194cbb8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.74      0.82        96\n",
      "           1       0.55      0.82      0.66        38\n",
      "\n",
      "    accuracy                           0.76       134\n",
      "   macro avg       0.73      0.78      0.74       134\n",
      "weighted avg       0.81      0.76      0.77       134\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucacolombo/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "knn_preds = knn_model.predict(X_val)\n",
    "#accuracy_score(knn_preds, y_val)\n",
    "print(classification_report(knn_preds, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2b448-affd-43ec-97e7-514965f577f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "374f1b49-ddd6-4b5e-b768-89787217f132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto',\n",
      " 'leaf_size': 30,\n",
      " 'metric': 'minkowski',\n",
      " 'metric_params': None,\n",
      " 'n_jobs': None,\n",
      " 'n_neighbors': 5,\n",
      " 'p': 2,\n",
      " 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "# parameter analysis is very similar to that done previously for the classification model\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knnreg_model = KNeighborsRegressor()\n",
    "pprint(KNeighborsRegressor().get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3eeac73f-81e9-404f-817e-4958264694dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29557543400826947"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnreg_model.fit(Xreg, yreg)\n",
    "knnreg_preds = knnreg_model.predict(Xreg_val)\n",
    "r2_score(yreg_val, knnreg_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be78f8-b1d9-46de-8a96-3dfad1c59829",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82f9cc-7658-482f-a15d-254b84136b18",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "<br>\n",
    "The objective of the SVM algorithm is to find a hyperplane in an N-dimensional space (where N is the number of features) that distinctly classifies the data.\n",
    "<br>\n",
    "For simplicity, we consider an example with 2 classes:\n",
    "<center><img src=\"images/svm/classes.png\" width=\"350\" center=/></center>\n",
    "As we can see we should split this two classes with many different hyperplanes, what do we choose?\n",
    "<center><img src=\"images/svm/hyperplanes.png\" width=\"350\" center=/></center>\n",
    "SVM works by searching for the hyperplane which maximizes the margin or the largest $\\gamma$ such that:\n",
    "$$\n",
    "\\forall{i,y_i}(wx_i + b) >= \\gamma\n",
    "$$\n",
    "<center><img src=\"images/svm/optimization.png\" width=\"350\" center=/></center>\n",
    "The support vectors are the data points that are closer to the hyperplane and influence the position and orientation of this one. In addition we can using only these support vectors to classify our new points instead of all the dataset. Using this techniques guaranteed a more robust classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4847d5-dfca-4fe2-9182-632b5ed3d32e",
   "metadata": {},
   "source": [
    "The advantages of support vector machines are:\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985a27b-9f73-45dc-9af8-0a545f5c2a2a",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4875aa1b-0b90-481d-8b39-657f45028aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0,\n",
      " 'break_ties': False,\n",
      " 'cache_size': 200,\n",
      " 'class_weight': None,\n",
      " 'coef0': 0.0,\n",
      " 'decision_function_shape': 'ovr',\n",
      " 'degree': 3,\n",
      " 'gamma': 'scale',\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'probability': False,\n",
      " 'random_state': None,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "# in our dataset we will be using SCV which is a specific type of SVM which is used with classification application\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "pprint(svc_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da7834-21d8-4f79-ba77-d080a905c12a",
   "metadata": {},
   "source": [
    "### Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdacf0-991b-44d2-99bf-bad1aea47ef3",
   "metadata": {},
   "source": [
    "the main parameters for SVC are:\n",
    "\n",
    "- kernel: it selects the type of hyperplane used to separate the data\n",
    "    - linear: a linear hyperplane\n",
    "    - rfb: a non-linear hyperplane\n",
    "    - poly: a non-linear hyperplane\n",
    "    - sigmoid\n",
    "- gamma: a parameter for non linear hyperplanes, higher the value more tries to fit the training data (risk overfitting)\n",
    "- C: regularization parameter, the penalty is a squared l2 penalty (higher more regularization, risk underfitting)\n",
    "- degree: parameter for poly kernel, we can choose the degree of the polynomial used for the hyperplane (using =1 is the same of use linear kernel, greater it fits more the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b260feb-2c3a-4da9-b81f-77bd108b1eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.79      0.82        85\n",
      "           1       0.68      0.78      0.72        49\n",
      "\n",
      "    accuracy                           0.78       134\n",
      "   macro avg       0.77      0.78      0.77       134\n",
      "weighted avg       0.79      0.78      0.79       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_model.fit(X, y)\n",
    "svc_preds = svc_model.predict(X_val)\n",
    "#accuracy_score(svc_preds, y_val)\n",
    "print(classification_report(svc_preds, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f5bbb-7fe2-402f-a5f3-c8de18145f24",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bf5b32a-7e73-4c1d-9580-09706390d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0,\n",
      " 'cache_size': 200,\n",
      " 'coef0': 0.0,\n",
      " 'degree': 3,\n",
      " 'epsilon': 0.1,\n",
      " 'gamma': 'scale',\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "# parameter analysis is very similar to that done previously for the classification model\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svmreg_model = SVR()\n",
    "pprint(svmreg_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c314220-d90f-474d-b883-4d8a8bc43d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13530293450130937"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmreg_model.fit(Xreg, yreg)\n",
    "svmreg_preds = svmreg_model.predict(Xreg_val)\n",
    "r2_score(yreg_val, svmreg_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b558026-5d5f-4798-8f33-9e462567b4f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# KRR (Kernel Ridge Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2f313-1548-47a2-8013-914d740e2017",
   "metadata": {
    "tags": []
   },
   "source": [
    "Kernel Ridge Regression models are nonparametric regression models which are able to deal with linear and nonlinear relationship. Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.\n",
    "<br>\n",
    "The form of the model learned by KRR is identical to SVR. Different loss function are used:\n",
    "- KRR: uses squared error loss\n",
    "- SVR: uses epsilon-intensive loss\n",
    "\n",
    "KRR is done in close-form so with medium datesets is faster than SVR. On the other hand, SVR scales better on larger datasets.\n",
    "\n",
    "## Note\n",
    "Kernel Trick: Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38db4044-bd70-40bb-b588-c2172f6b5e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1,\n",
      " 'coef0': 1,\n",
      " 'degree': 3,\n",
      " 'gamma': None,\n",
      " 'kernel': 'linear',\n",
      " 'kernel_params': None}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "krr_model = KernelRidge()\n",
    "pprint(krr_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbd548-a7ca-4ce2-a7c8-0a5f03bea629",
   "metadata": {},
   "source": [
    "## Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43f0a5-802f-492a-a771-2bcac806c3a1",
   "metadata": {},
   "source": [
    "The main parameters for Kernel Ridge Regression are:\n",
    "- alpha: regularization coefficient. Larger the value, greater regularization\n",
    "- kernel: kernel mapping used internally (use documentation)\n",
    "- gamma: parameters for some kernels\n",
    "- degree: degree of the polynomial kernel (ignored by other kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4c410d5-d079-469f-9282-fb0a2d533599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4418097164269671"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krr_model.fit(Xreg, yreg)\n",
    "krr_preds = krr_model.predict(Xreg_val)\n",
    "r2_score(yreg_val, krr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b485f-8151-44f7-b156-d98a4059990a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c91645-3f0f-4f8e-9507-319d13020caf",
   "metadata": {},
   "source": [
    "## Bayesian Regression\n",
    "Before we need to introduce the idea of the Bayesian Regression. In the Bayesian, we formulate linear regression using probability distributions rather than point estimates. The target y, is not estimated as a single value, but it is assumed to be drawn from a probabilistic distribution. So each y is seen as a sample, which is sampled from a normal distribution, where the normal distribution parameters are treated as random variable which are estimated from the data:\n",
    "$$\n",
    "y = \\mathcal{N}(W^T X,\\,\\sigma^{2}I)\n",
    "$$\n",
    "Where:\n",
    "- the mean: $W^T$ weight multiplied by the data points\n",
    "- the variance is the square of the standard deviation multiplied by the Identity matrix (because we have multidimensional data)\n",
    "<br>\n",
    "The aim of Bayesian Linear Regression is not to find the single “best” value of the model parameters, but rather to determine the posterior distribution for the model parameters. Using the Bayesian' rules:\n",
    "$$\n",
    "p(W | data) = \\frac{p(data | W) * p(W)}{p(data)}\n",
    "$$\n",
    "where:\n",
    "- $p(W | data)$ is the posterior probability of parameters W given training data\n",
    "- $p(data | W)$ is the probability (likelihood) of observing D given W\n",
    "- $p(W)$ is the prior probability over the parameters\n",
    "- $p(data)$ is the marginal likelihood (normalizing constant)\n",
    "\n",
    "We want the most probable value of W given the data: maximum a posteriori (MAP). It is the mode of the posterior.\n",
    "<br>\n",
    "To clarify the idea:\n",
    "- Priors: If we have domain knowledge, or a guess for what the model parameters should be, we can include them in our model, unlike in the frequentist approach which assumes everything there is to know about the parameters comes from the data. If we don’t have any estimates ahead of time, we can use non-informative priors for the parameters such as a normal distribution.\n",
    "- Posterior: The result of performing Bayesian Linear Regression is a distribution of possible model parameters based on the data and the prior. This allows us to quantify our uncertainty about the model: if we have fewer data points, the posterior distribution will be more spread out.\n",
    "\n",
    "As the amount of data points increases, the likelihood washes out the prior, and in the case of infinite data, the outputs for the parameters converge to the values obtained from OLS.\n",
    "<br>\n",
    "Here a visual examples:\n",
    "<center><img src=\"images/brr/blr.png\" width=\"450\" center=/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba00623-97c5-4a30-99bd-859326a1d38d",
   "metadata": {},
   "source": [
    "## Bayesian Ridge\n",
    "Bayesian Ridge estimates a probabilistic model of the regression problem as described above but the prior for the coefficient is given by a spherical Gaussian, so used the idea of the ridge regression instead only of the linear regression.\n",
    "The ridge minimize the classical linear least squared adding the l2 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d381a00-e77b-4fe0-802b-33fc8bdaf5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha_1': 1e-06,\n",
      " 'alpha_2': 1e-06,\n",
      " 'alpha_init': None,\n",
      " 'compute_score': False,\n",
      " 'copy_X': True,\n",
      " 'fit_intercept': True,\n",
      " 'lambda_1': 1e-06,\n",
      " 'lambda_2': 1e-06,\n",
      " 'lambda_init': None,\n",
      " 'n_iter': 300,\n",
      " 'normalize': 'deprecated',\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "brr_model = BayesianRidge()\n",
    "pprint(brr_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1ba44-9504-45fd-8d39-494cac7d12eb",
   "metadata": {},
   "source": [
    "## Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11b476-09c1-40f1-933e-0ed6a6bc03de",
   "metadata": {},
   "source": [
    "The main parameters for Bayesian Ridge Regression are:\n",
    "- n_iter: max number of iteration\n",
    "- tol: convergence parameter, if w changes < tol stop the the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c55c79-3531-4b8f-a308-b0ff248934af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4700037663317521"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brr_model.fit(Xreg, yreg)\n",
    "brr_preds = brr_model.predict(Xreg_val)\n",
    "r2_score(yreg_val, brr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfcb719-77c3-4cd6-ad94-3a5cee9f3b11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ElasticNet Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8afb1-e811-4262-83cf-93ee0d8aeea4",
   "metadata": {},
   "source": [
    "ElasticNet Regression is a regularized regression method which linearly combines the l1 (Lasso) and l2 (Ridge) penalties. The object function will be:\n",
    "$$\n",
    "J(W) = \\frac{1}{N}\\sum_{i}^{N}{(y - \\hat{y})^2} + r\\lambda\\sum_{j}^{M}{|w_j|} + \\frac{1 - r}{2}\\lambda\\sum_{j}^{M}{w_j^2}\n",
    "$$\n",
    "where:\n",
    "- $\\frac{1}{N}\\sum_{i}^{N}{(y - \\hat{y})^2}$: the mean squared error loss function <p>\n",
    "- $r\\lambda\\sum_{j}^{M}{|w_j|}$: l1 penalty (Lasso) <p>\n",
    "- $\\frac{1 - r}{2}\\lambda\\sum_{j}^{M}{w_j^2}$: l2 penalty (Ridge) <p>\n",
    "\n",
    "Elastic net regression acts as both lasso and ridge, and we can control its behavior with a hyperparameter known as L1 ratio, mathematically denoted as ‘r’. A value of r is set between 0 and 1. If r=0.5,  elastic net behaves like Lasso and Ridge equivalently. But, when we start decreasing the value of r from 0.5 to 0, Elastic net behaves roughly as Ridge, but at 0, it completely behaves as Ridge regression. If the value of r changes from 0.5 to 1, Elastic net behaves like Lasso, but at 1, it will completely behave as Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87c9387-6b89-46ad-8131-f305a9278bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0,\n",
      " 'copy_X': True,\n",
      " 'fit_intercept': True,\n",
      " 'l1_ratio': 0.5,\n",
      " 'max_iter': 1000,\n",
      " 'normalize': 'deprecated',\n",
      " 'positive': False,\n",
      " 'precompute': False,\n",
      " 'random_state': None,\n",
      " 'selection': 'cyclic',\n",
      " 'tol': 0.0001,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en_model = ElasticNet()\n",
    "pprint(en_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a776624-8c55-4be5-9d78-a6d7be0759ac",
   "metadata": {},
   "source": [
    "## Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b482de2-5345-45b1-ab3a-3ebc573ae2ac",
   "metadata": {},
   "source": [
    "The ElasticNet main parameters are:\n",
    "- l1_ratio: the elastic mixing parameter. For $l1\\_ratio = 0$ the penalty is an L2 penalty. For $l1\\_ratio = 1$ it is an L1 penalty. For $0 < l1\\_ratio < 1$, the penalty is a combination of L1 and L2.\n",
    "- alpha: Constant that multiplies the penalty terms. $alpha = 0$ is equivalent to an ordinary least square (as linear regression problem)\n",
    "- max_iter: max number of iterations\n",
    "- tol: tolerance for optimization convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba83eb9-f4d5-48a8-80c3-190614096a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47673568102491004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.fit(Xreg, yreg)\n",
    "en_preds = en_model.predict(Xreg_val)\n",
    "r2_score(yreg_val, en_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e30fba-1091-4be8-bdd1-acf9cf2d2839",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SGD Classifier/Regression (Stochastic Gradient Descent Classifier/Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d766b-4ac7-4460-a4a5-4853f7d8bb16",
   "metadata": {},
   "source": [
    "This could make people misunderstand, in this case SGD is not the optimizer. Instead, it is a linear classifier (we can choose between SVM, logistic regression, ...), which is optimized using the SGD algorithm.\n",
    "<br>\n",
    "The advantages of Stochastic Gradient Descent are:\n",
    "- Efficiency.\n",
    "- Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "The disadvantages of Stochastic Gradient Descent include:\n",
    "- SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "- SGD is sensitive to feature scaling.\n",
    "\n",
    "<center><img src=\"images/sgd.gif\" width=\"450\" center=/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf7123-f6da-4ce4-bb7e-e7a9e8571bde",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "393f77b5-fb0c-40bc-a17f-f1edb9174ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001,\n",
      " 'average': False,\n",
      " 'class_weight': None,\n",
      " 'early_stopping': False,\n",
      " 'epsilon': 0.1,\n",
      " 'eta0': 0.0,\n",
      " 'fit_intercept': True,\n",
      " 'l1_ratio': 0.15,\n",
      " 'learning_rate': 'optimal',\n",
      " 'loss': 'hinge',\n",
      " 'max_iter': 1000,\n",
      " 'n_iter_no_change': 5,\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'power_t': 0.5,\n",
      " 'random_state': None,\n",
      " 'shuffle': True,\n",
      " 'tol': 0.001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgdc_model = SGDClassifier()\n",
    "pprint(sgdc_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd3610-33a1-4fe8-a735-4d9941079cca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c21655-5bc2-47d6-b4c7-f9519264ed5e",
   "metadata": {},
   "source": [
    "SGD Classifier main parameters:\n",
    "- loss: the loss function to be used\n",
    "    - hinge: gives a linear SVM\n",
    "    - log_loss: gives logistic regression (probabilistic classifier)\n",
    "    - look doc for others\n",
    "- penalty: the regularization term to be used\n",
    "    - l2\n",
    "    - l1\n",
    "    - None\n",
    "- alpha: constant that multiplies the regularization term (higher value -> stronger regularization)\n",
    "- max_iter: the maximum number of iterations for the sgd algorithm (the epochs)\n",
    "- tol: the stopping criterion, raining will stop when (loss > best_loss - tol). Convergence is checked against the training loss or the validation loss depending on the early_stopping parameter\n",
    "- early_stopping: Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score returned by the score method is not improving by at least tol for n_iter_no_change consecutive epochs.\n",
    "- learning_rate: learning rate schedule\n",
    "    - ‘constant’: eta = eta0\n",
    "    - ‘optimal’: eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "    - ‘invscaling’: eta = eta0 / pow(t, power_t)\n",
    "    - ‘adaptive’: eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5.\n",
    "- eta0: the initial_learning for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.\n",
    "- validatio_fraction: The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aaaa9a4-7766-444c-8428-77e1311b398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.80        96\n",
      "           1       0.54      0.79      0.64        38\n",
      "\n",
      "    accuracy                           0.75       134\n",
      "   macro avg       0.72      0.76      0.72       134\n",
      "weighted avg       0.79      0.75      0.76       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgdc_model.fit(X, y)\n",
    "sgdc_preds = sgdc_model.predict(X_val)\n",
    "#accuracy_score(sgdc_preds, y_val)\n",
    "print(classification_report(sgdc_preds, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d11e2d-6a8a-4330-b2a1-bfa7eae42df9",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddb023ef-82b5-46a3-8ec6-b2b018e75319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001,\n",
      " 'average': False,\n",
      " 'early_stopping': False,\n",
      " 'epsilon': 0.1,\n",
      " 'eta0': 0.01,\n",
      " 'fit_intercept': True,\n",
      " 'l1_ratio': 0.15,\n",
      " 'learning_rate': 'invscaling',\n",
      " 'loss': 'squared_error',\n",
      " 'max_iter': 1000,\n",
      " 'n_iter_no_change': 5,\n",
      " 'penalty': 'l2',\n",
      " 'power_t': 0.25,\n",
      " 'random_state': None,\n",
      " 'shuffle': True,\n",
      " 'tol': 0.001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# parameter analysis is very similar to that done previously for the classification model\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgdreg_model = SGDRegressor()\n",
    "pprint(sgdreg_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2877690e-e23e-4ed4-b41c-97de1935ccb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8819985953312979e+25"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdreg_model.fit(Xreg, yreg)\n",
    "sgdreg_preds = sgdreg_model.predict(Xreg_val)\n",
    "r2_score(yreg_val, sgdreg_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add8a6a-14de-4f18-9445-5bc846792b8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85f64e-00c9-436b-87ea-29f89c4c5a81",
   "metadata": {},
   "source": [
    "Decision trees are non-parametric methods for classification and regression. The object is to create a model that predicts the value of the target by learning simple decision rules inferred from the data features.\n",
    "<br>\n",
    "Here report a categorical example:\n",
    "<center>\n",
    "    <img src=\"images/decision_trees/dataset_tree_cat.png\" width=\"400\" center=/>\n",
    "    <img src=\"images/decision_trees/cat_tree.png\" width=\"360\" center=/>\n",
    "</center>\n",
    "<br>\n",
    "<br>\n",
    "To better understand the geometric interpretation of a decision tree, consider the following example:\n",
    "<br>\n",
    "For simplicity, here we have two numerical variable $x_1$ and $x_2$ and what to classify two classes, the final trees look like this:\n",
    "<center><img src=\"images/decision_trees/numerical_tree.png\" width=\"350\" center=/></center>\n",
    "The geometrical interpretation is the following:\n",
    "<center><img src=\"images/decision_trees/geometrical_tree.png\" width=\"350\" center=/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11102aca-1f2c-4942-8f63-66a087923d1e",
   "metadata": {},
   "source": [
    "Some advantages of decision trees are:\n",
    "- Simple to understand and to interpret. Trees can be visualized.\n",
    "- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "- Able to handle both numerical and categorical data.\n",
    "- Able to handle multi-output problems.\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e27fb3be-fd37-4701-a1c5-01db4d19f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'random_state': None,\n",
      " 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "pprint(dt_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd141c-c96c-4b87-a3d6-c3be53c7aa12",
   "metadata": {},
   "source": [
    "## Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc722e8-9b07-41fc-b509-ee19377e8249",
   "metadata": {},
   "source": [
    "The main parameters for Decision Trees are:\n",
    "- criterion: the function to evaluate the quality of a split, the main ones:\n",
    "    - entropy: $H(E) = -\\sum{p_j\\log p_j}$\n",
    "    - gini: $Gini(E) = 1 - \\sum{p_j^2}$ \n",
    "- splitter: the strategy used to make the split\n",
    "    - best: best feature based on the impurity measure \n",
    "    - random: select a random feature\n",
    "- max-depth: maximum depth of the tree, if None it expands until leave are pure (risk of overfitting)\n",
    "- min_sample_split: the minimum number of samples required to split an internal node, rule of thumb [1, 40], this value is used to control overfitting\n",
    "- mean_sample_leaf: the minimum number of samples required to be at a leaf node, rule of thumb [1, 20]\n",
    "- max_features: the number of features to consider when looking for the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96ca2e3d-d5c6-48d5-9654-c984ebf02ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.78        83\n",
      "           1       0.64      0.71      0.67        51\n",
      "\n",
      "    accuracy                           0.74       134\n",
      "   macro avg       0.73      0.73      0.73       134\n",
      "weighted avg       0.74      0.74      0.74       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_model.fit(X, y)\n",
    "dt_preds = dt_model.predict(X_val)\n",
    "#accuracy_score(dt_preds, y_val)\n",
    "print(classification_report(dt_preds, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81096b4-9d25-4899-af21-584a7c1b668b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cdec8-d540-4017-b85d-2db2c629a7f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ensemble Methods\n",
    "Before analyzing the Random Forest, I have to introduce the concept of ensemble methods, there are two main kind of these:\n",
    "- Bagging\n",
    "- Boosting\n",
    "\n",
    "### Bagging Method\n",
    "Bagging is an aggregation of independent generating models of the same kind, where the prediction is the combined voting/average of all the models. Baggings is a good idea when it is used to combine unstable classifier such as decision trees, regression trees, linear regression, neural networks:\n",
    "- it is used to stabilize the unstable classifier, aggregating them\n",
    "- it reduces the overall variance and keeping the bias unchanged\n",
    "- reduce the overfitting of these high variance classifier (if they are taken separately)\n",
    "- each new model is independent of the other\n",
    "- the training data is random with bootstrap\n",
    "\n",
    "### Boosting Method\n",
    "Boosting merges a sequence of simple classifier by correcting itself over the misclassified data. As said it is a good idea when using stable and simple classifier such as KNN or model with strong regularizations:\n",
    "- it reduces the overall bias, keeping variance unchanged\n",
    "- each model is dependent of the previous one when trained\n",
    "- the training data is the misclassified previus dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e227b-03a6-4781-a01c-d9f01c57a6f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest Classifier/Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab7043-1bed-4d55-b9b5-d0417b5fa437",
   "metadata": {},
   "source": [
    "Random Forests are an application of the bagging method. They are ensembles of unpruned decision tree learners with randomized selection of features at each split.\n",
    "<br>\n",
    "The generalization error of a forest of tree classifiers depends on:\n",
    "- The strength of the individual trees in the forest\n",
    "- The correlation between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe39ca5-b477-4201-98d4-c9b13c33bf71",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a33bc017-b0d4-4377-88b7-9005cde0153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "pprint(rf_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900fc7e-861f-456c-8338-c8e21a92d372",
   "metadata": {},
   "source": [
    "#### Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bdd3a-9c46-42ac-ad30-a7ab785399ad",
   "metadata": {},
   "source": [
    "These are a lot of parameters, we are interested in:\n",
    "- criterion: function used to measure the quality of a split for each tree, main\n",
    "    - entropy\n",
    "    - gini\n",
    "- n_estimators: number of trees in the forest\n",
    "- max_features: max number of features considered for splitting a node\n",
    "- max_depth: max number of levels in each decision tree\n",
    "- min_samples_split: min number of data points placed in a node before the node is split\n",
    "- min_samples_leaf: min number of data points allowed in a leaf node\n",
    "- bootstrap: method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72517d9e-1fbe-451b-a3e3-67c4c1b6e2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77        77\n",
      "           1       0.70      0.68      0.69        57\n",
      "\n",
      "    accuracy                           0.74       134\n",
      "   macro avg       0.73      0.73      0.73       134\n",
      "weighted avg       0.74      0.74      0.74       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "rf_model.fit(X, y)\n",
    "rf_preds = rf_model.predict(X_val)\n",
    "#accuracy_score(rf_preds, y_val)\n",
    "print(classification_report(rf_preds, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b78efad8-ed01-48c0-8898-61f2600b8abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print feature importances created in randomForest\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "importances\n",
    "forest_importances = pd.Series(importances, index=X.columns)\n",
    "forest_importances = forest_importances.sort_values()\n",
    "forest_importances.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710906a3-c87b-4b78-a7ae-e3b0a763a0cd",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4ef59c8-ba3d-4741-8bec-01185667ee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'squared_error',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# parameter analysis is very similar to that done previously for the classification model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfreg_model = RandomForestRegressor()\n",
    "pprint(rfreg_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecb76703-2df2-4a32-b14c-cb817822c9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6370227935137224"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg_model.fit(Xreg, yreg)\n",
    "rfreg_preds = rfreg_model.predict(Xreg_val)\n",
    "r2_score(yreg_val, rfreg_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8c82a-0aca-479d-9dbb-d882159d77b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RandomForest Search Grid Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2070dc-7d6f-4cfb-a9ef-ccece8b4fc98",
   "metadata": {},
   "source": [
    "### Random Hyperparamter Grid & Searching\n",
    "We use an initial random grid to find a closer range, so later we can apply the grid search in a lower range search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce62c989-cc02-4cd0-85b9-291c94cf26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=1000, num=10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(start=5, stop=50, num=10)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# create the random grid\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators' : n_estimators,\n",
    "    'max_features' : max_features,\n",
    "    'max_depth' : max_depth,\n",
    "    'min_samples_split' : min_samples_split,\n",
    "    'min_samples_leaf' : min_samples_leaf,\n",
    "    'bootstrap' : bootstrap\n",
    "}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9a539a1-9c7b-4710-9651-972a67b4768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [5, 10, 15, 20, 25, 30, 35,\n",
       "                                                      40, 45, 50, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can do the search on the training set\n",
    "# we use 3 fold cross validation over 100 different combinations\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                               n_iter=100, cv=3, verbose=2, n_jobs=-1)\n",
    "rf_random.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b60831f-606e-4094-8c35-d9e36ca95fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 40,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "702f2d14-2cdb-4a03-8e0b-eccc0236fe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746268656716418"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "pred = best_random.predict(X_val)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a7c08-5f99-442d-94f4-9c3a113cc149",
   "metadata": {},
   "source": [
    "### Grid Search with Cross Validation\n",
    "Random search allowed us to narrow down the range for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39e5c415-1f39-46eb-938c-7e21da763347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5, 25, 50],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'n_estimators': [100, 300, 1000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f607b169-501a-46ba-a0f7-b59be0a5d2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [5, 25, 50],\n",
       "                         'max_features': ['auto'], 'min_samples_leaf': [1, 2],\n",
       "                         'min_samples_split': [2, 5],\n",
       "                         'n_estimators': [100, 300, 1000]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fea4ee11-48c6-46f4-a195-b3b12220fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 25,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4228788d-087c-4437-b634-1276ef0ee875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7388059701492538"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_preds = best_grid.predict(X_val)\n",
    "accuracy_score(grid_preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59682a-814c-4665-9592-36b5c0b1f450",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e9e8a-6794-42f1-9452-cd7a5989f3c4",
   "metadata": {},
   "source": [
    "Gradient boosting is one of the variants of ensemble methods where you create multiple weak models and combine them to get better performance as a whole.\n",
    "<br>\n",
    "How it works? Build a sequence of predictors by repeating three simple steps:\n",
    "1. Learn a regression predictor\n",
    "2. Compute the error residual\n",
    "3. Learn to predict the residual \n",
    "4. Goto 2\n",
    "<br>\n",
    "Why is it called Gradient Boosting?\n",
    "- To compute a model to predict a target value yi that minimizes a loss function, for instance the mean square error\n",
    "$$\n",
    "MSE(y, \\hat{y}) = \\frac{1}{N}\\sum{(y_i - \\hat{y_i})^2}\n",
    "$$\n",
    "- We can adjust $\\hat{y_i}$ to try to reduce the error using the gradient:\n",
    "$$\n",
    "\\hat{y_i} = \\hat{y_i} + \\alpha\\nabla{MSE(y,\\hat{y})}\n",
    "$$\n",
    "- The gradient for the MSE is a function of:\n",
    "$$\n",
    "y_i - \\hat{y_i}\n",
    "$$\n",
    "- Each learner is estimating the gradient of the loss. Larger $\\alpha$ means larger steps, smaller $\\alpha$ smaller steps and smoothing effect\n",
    "\n",
    "<center><img src=\"images/gb/GB.gif\" width=\"350\" center=/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98951382-bb9b-47c6-941a-670cc5ab317a",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4051e8a3-b659-4e04-bb97-a10f4b3a40e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'deviance',\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_iter_no_change': None,\n",
      " 'random_state': None,\n",
      " 'subsample': 1.0,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# sklearn.ensemble.HistGradientBoostingClassifier is a much faster variant of this algorithm \n",
    "# for intermediate datasets (n_samples >= 10_000).\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc_model = GradientBoostingClassifier()\n",
    "pprint(gbc_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2920de-bf21-406b-bb3e-c60dde39e262",
   "metadata": {},
   "source": [
    "### Main Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de69c2c-91c6-4e84-b595-6ee8c5799e75",
   "metadata": {},
   "source": [
    "The main parameters of Gradient Boosting algorithms are:\n",
    "- loss: loss function to be optimized\n",
    "- learning_rate\n",
    "- n_estimators: The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance\n",
    "- min_samples_split: The minimum number of samples required to split an internal node\n",
    "- min_samples_leaf: The minimum number of samples required to be at a leaf node\n",
    "- max_depth: Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2f30ea3-376b-44fb-a2aa-eda41a8a151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76        80\n",
      "           1       0.64      0.67      0.65        54\n",
      "\n",
      "    accuracy                           0.72       134\n",
      "   macro avg       0.71      0.71      0.71       134\n",
      "weighted avg       0.72      0.72      0.72       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbc_model.fit(X, y)\n",
    "gbc_preds = gbc_model.predict(X_val)\n",
    "#accuracy_score(gbc_preds, y_val)\n",
    "print(classification_report(gbc_preds, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfcb20-0fea-446f-a54b-2f10cd4720eb",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb65c78b-51b8-41fc-9ff2-a99d9c60e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.9,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'squared_error',\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_iter_no_change': None,\n",
      " 'random_state': None,\n",
      " 'subsample': 1.0,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# parameter analysis is very similar to that done previously for the classification model\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbreg_model = GradientBoostingRegressor()\n",
    "pprint(gbreg_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed11f849-270b-440a-89b0-2c7f4822031b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.694527377267204"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbreg_model.fit(Xreg, yreg)\n",
    "gbreg_preds = gbreg_model.predict(Xreg_val)\n",
    "r2_score(yreg_val, gbreg_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9996a9-b56c-4e38-b36e-e1569c3b5cfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost\n",
    "Optimized Gradient Boosting algorithm through parallel processing, tree-pruning, handling missing values and regularization to avoid overfitting/bias.\n",
    "<br>\n",
    "source: https://xgboost.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d1d97-7d7c-4bab-90fe-05aa6a14fcec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LGBM\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "- Faster training speed and higher efficiency.\n",
    "- Lower memory usage.\n",
    "- Better accuracy.\n",
    "- Support of parallel, distributed, and GPU learning.\n",
    "- Capable of handling large-scale data.\n",
    "\n",
    "source: https://lightgbm.readthedocs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd258767-582c-42f3-983a-2bfa77227243",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "CatBoost is an algorithm for gradient boosting on decision trees. Its main advantages are:\n",
    "- Great quality without parameter tuning\n",
    "- Categorical features support\n",
    "- Fast and scalable GPU version\n",
    "- Improved accuracy\n",
    "- Fast prediction\n",
    "\n",
    "source: https://catboost.ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
